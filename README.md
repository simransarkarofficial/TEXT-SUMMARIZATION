# TEXT-SUMMARIZATION

I fine-tuned the T5ForConditionalGeneration model to effectively summarize dialogue-based text. The input text was preprocessed using tokenization and padding, preparing it for transformer-based training. Leveraging PyTorch and GPU acceleration in Google Colab, I trained the model on a custom dataset. After training, I saved the fine-tuned model and tokenizer locally for seamless deployment in VSCode. The model was tested on sample dialogues, successfully generating concise and meaningful summaries.
